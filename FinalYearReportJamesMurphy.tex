\documentclass[12pt]{article}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}
\usepackage{parskip}
\usepackage{graphicx}
\begin{document}
\begin{titlepage}

\title{Music Recommendation through Collaborative Filtering, assisted by Location based Services}

\author{James Murphy}


\maketitle
\vspace{4cm}
\begin{center}

\textbf{Supervisor: } Dr. Derek Bridge\\
\textbf{Second Reader:} TBD
\end{center}





\end{titlepage}

\pagebreak
\section*{Acknowledgements}
I'd like to thank Soundwave for their data. etc  
\pagebreak
\section*{Declaration of Originality}

In signing this declaration, you are confirming, in writing, that the submitted work is entirely your own original work, except where clearly attributed otherwise, and that it has not been submitted partly or wholly for any other educational award.

I hereby declare that:-
\begin{itemize}
\item With respect to my own work: none of it has been submitted at any
educational institution contributing in any way towards an educational
award; 

\item With respect to another’s work: all text, diagrams, code, or ideas,
whether verbatim, paraphrased or otherwise modified have
been duly attributed to the source in a scholarly manner, whether from
books, papers, lecture notes or any other students work, whether published or unpublished, electronically or in print.  

\item This is all my own work, unless clearly indicated otherwise, with full
and proper accreditation;



\end{itemize}

\vspace{2cm}

Name: James Murphy

\vspace{1cm}

Signed:

\vspace{1cm} 
Date: 04/04/2014
\pagebreak
\section*{Abstract}
Gracenote, the industry recognised largest source of music meta data , has stated it has over 180 million tracks \cite{Gracenote2014} in it's database.  It stands to reason with that number of songs, that finding a song to listen to at any given point time is quite an ordeal. Knowing which song to listen to, is becoming overwhelming. As a consequence of this, people have started to look for a narrower selection of songs to choose from; Song's recommended just for their specific tastes. 

It's from this observation that music recommenders have started popping up everywhere. From YouTube, to Spotify, to your very own iPod being able to recommend songs to you from your library through a service called iTunes Genius. In this report, the various techniques music recommenders currently implement will be analysed, along with pioneering some new techniques.

\pagebreak

\tableofcontents
\pagebreak
\textbf{•}
\listoffigures

\pagebreak
\section{Introduction}
\subsection{Music - An ever expanding industry}

Ever since the birth of man, music has had an ever-present nature. In 1877, when Thomas Edison invented the Phonograph, he gave people the first opportunity to both record and listen to music. Such an important milestone cannot be overstated. People could now record themselves, distribute their music, and maybe as a by-product, become known publicly as an 'Artist'. Jump forward over a century to the present, and the music industry generated over \$16.5 billion in 2012 \cite{ifti}.

The multiple ways in which music can be obtained now are greater than ever. Online music stores have now become the biggest avenue to obtain music. But even within that one discipline, there is a vast amount of choice. Wikipedia's article on Online Music stores compares 42 major online stores alone. Just one of these, iTunes, recorded sales of over \$4.3 Billion in one quarter. \cite{itunes} Not restricted to having physical copies of the records they sell, online stores can store much more music than regular stores. iTunes, as an example, has over 37 million tracks \cite{itunesFeatures}. 

In recent years, massive growth in online subscription services had led to more and more people availing of this cheap new way to listen to music. The number of people paying to use subscription services grew 44 per cent in 2012 to 20 million. Subscription revenues are expected to account for more than 10 per cent of digital revenues for the first time in 2012 \cite{ifti}. With all these services popping up, people now have more choice over to which music to listen to, and it is easier than ever to listen to it.

The only drawback is, this exacerbates a problem which already resides in regular mortar and brick record stores - What should I buy? This, is where music recommendation comes in. 
\pagebreak
\subsection{What makes music special}
Blah blah Music machinery 
\subsection{Project Aim}
The aim of this project is to explore the realm of music recommendation. This was done to be done with regards to a new dataset that had never been tested on, and to support this dataset, an existing dataset ( last.fm ) was used as a base case. Output from this could be trusted as ground truth on whether an experiment was working properly or not, as this data is well documented and tested previously. The recommendation goals were to see if we could use this data to recommend users tracks to listen to effectively. Firstly using commonly used methods, and consequently to then use some new recommending techniques, and see how well they performed.
\pagebreak

\section{Recommending Techniques Used}

	\subsection{Collaborative Filtering}
Collaborative Filtering is widely used throughout our society. In a world where everything is getting more and more tailor-fit to the customer, collaborative filtering is one of the main approaches used to achieve this custom-fitting of goods. It is most simply described, by it's founders, Goldberg, Nochols, Oki and Terry. \cite{goldberg1992using}


\begin{quote}
\textit{Collaborative filtering simply means that people 
collaborate to help one another perform filtering by 
recording their reactions to documents they read.}
\end{quote}


This quote was in reference to the model that Goldberg et Al used Collaborative Filtering for first - a mailing system. They described  that the main theory of Tapestry(cite), their mailing system, was that more effective filtering can be done by involving humans in the process. Indeed, using the reviews and thoughts of the mass, to decide on one individual, is an area being thoroughly harvested in all facets of life now. 

Collaborative filtering so, is applicable in many areas. The only thing that differs is the input. To narrow down the scope of these applications relative to this report, it is necessary to state the uses made of collaborative filtering. Collaborative filtering in this report, is used in the context of unary data. This unary data is whether a given user played a given track. This method of using CF (Collaborative Filtering) makes evaluations quick, and simple manipulation of data.

Collaborative filtering is highly applicable to Music datasets. 
Since music can generally be categorised into different genre groups (Indie, pop, Jazz), or indeed even at an artist level ( Ellie Goulding, Macklemore), the system can start to use these definitions to recommend to people. For example, say person A loves pop, and Jazz, and has played Ellie Goulding. You also love pop, Jazz, and have too played Ellie Goulding, the system can start to recommend you some of the tracks user A has played that you haven't. 
\todo{ More concrete example needed}


There are two main types of Collaborative Filters being used currently, User-User filters, and Item-Item filters. These will be discussed in detail in section 4.2.





	\subsection{Location Based Filtering}
	
Fill in when completed

\pagebreak
\section{Datasets}
	\subsection{Soundwave 161k}
	161k actions, 42k users, 796 over 20 plays
	
	
	avg and std dev of users plays
	\subsection{Soundwave 1.3 million}
	1.3mil actions, 73k users, 18000 over 20 plays
	
	
	avg and std dev of users plays
	\subsection{Last.fm}
	
	1 million songs
	
\pagebreak	
\section{Collaborative Filtering}
	
	\subsection{ Recommender Types}
		\subsubsection{User-User Recommender}
A user-user recommender operates using user profiles as a means of filtering. To make a recommendation for a user, the system take the songs that that user has played, and see which users in the dataset are most similar to this user. This similarity was calculated in three ways, set intersection, Pearson Correlation and Cosine Similarity. After taking the top-N number of users, the system gathers up all their songs which the user hasn't played and find the most suitable songs to recommend to the user. More detail on how the most suitable songs are picked is given in the implementation section. \todo{expand}
		\subsubsection{Item-Item Recommender}
An item-item recommender is quintessentially the opposite of a User-User recommender. Instead of getting similar users, the system get similar items and recommend those. Items can be classed as similar in terms of the users that have played said items. The aforementioned similarity measures can again be used to measure degrees of similarity. To recommend to one user, the system takes their tracks and item-item recommend for each of the tracks and compile the recommendations into a list. More on the mechanics of the final songs to be recommended is given in the implementation. \todo{expand}
\subsubsection{Item Recommender with User-User rerank}
This recommender is a slight variation of the Item-Item recommender. Instead of being used to recommend for a user initially, like the Item-Item does, this simply gives recommendations for a given input song. The twist is that the recommendations given from this, are then used as candidate songs for a user-user rerank . The songs will be reranked, in terms of similarity of the user using the system. So the end result, is a personalised recommendation list on a given input song.

\todo{Insert histogram of Spearman Coefficient here}
		
		
	\subsection{Limitations}
		\subsubsection{Push Down}
		When we are testing a recommenders output, the best case scenario is that we have the actual current user we are testing present, to confirm whether a given recommendation is good or not. However in our tests, since we were always working with just data, we just had to assert from that persons past playing history if the recommendation was good or not. This is called offline evaluation. 
		
		The problem with this, is that the recommender could recommend a perfectly good song, one the user would love to have played, but since they haven't played it previously, the recommender tester will punish the system as it thinks it is a bad recommendation. It will not be in the users test data set, and therefore will evaluate to a bad recommendation.The best metric to use, is to gauge how people reacted to their recommendation, and score the recommender accordingly. 
		
		It is these good recommendations, that the system does not know about for that specific user, that push down the other recommendations that the system makes, 
		
		 These hidden data evaluations penalise algorithms that actually find new data for a user, and it is for reasons like this, that real  life user testing is necessary for proper evaluation of a system.
		


		\subsubsection{Dead Data}
All the data that we worked with on this project is what is known as dead data. It is a dump of playing history from one source or another, which documents what tracks users played at one point or another. Whilst working with this retrospective look at the data is easier in the fact that it is static and can be reused, it has some quite important limitations. 

One of these is the previously mentioned, push-down effect. Where tracks not known to the system push down high scoring recommendations, and therefore impact the assigned system rating. 

Another is 
		\subsubsection{Play Vs Like}
		 Just because a user played something, does not mean they like it, or want to hear it again. They could have been too lazy to change the next song, or just have been in a particular mood in which they tolerated that song. That means for all future evaluations, this song will have some input, when in reality, the user doesn’t even like this song. Because we are just dealing with plays in these datasets, and not likes also, we can never be fully sure that a user like the song they are playing, or ever want to hear it again. Play data is extremely noisy in that respect.
		 \subsubsection{Gray Sheep/Black Sheep}
		 
		 \subsubsection{Synonymy}
		 
		 \subsubsection{Cold Start/Low number of plays} 
	\subsection{Algorithms}
		\subsubsection{K-Nearest Neighbours}
\pagebreak
\section{Location Based Filtering}
	\subsection{Recommender Types}
		\subsubsection{Location based Recommender}
		\subsubsection{Geofence based Recommender}
	\subsection{Limitations}
		\subsubsection{Synonymy with echonest}
		\subsubsection{Choosing of Epsilon value}
	\subsection{Clustering Algorithms}
		\subsubsection{DBSCAN}
			Density based algorithm		 
\pagebreak
\section{Implementation}
	\subsection{Recommenders}
		\subsubsection{User-User}
		\subsubsection{Item-Item}
		\subsubsection{Item rerank}
		\subsubsection{Location Based}
		\subsubsection{Geofence Based}
	\subsection{Calculating Similarity}
		  \subsubsection{Set Intersection}
			The simplest for of measuring how close two sets of data are, is seeing how many things they have in common. This is what set intersection does. It takes the number of the items in the current set, compares this to the data set currently being tested against and notes the songs in common - the set intersection. These are then ranked by the sets which have the highest set intersection, the most similar data sets according to this similarity measure.
			
Of the three measures, this is the most primitive, and thus is the first measure we used when conducting our tests. 			   		  
		 \todo{ graph on accuracy}
		  \subsubsection{Pearson Correlation}
Pearson Correlation is a slightly more complicated formula, and measure of similarity. It is defined in the following notation, and usually represented by the letter 'r':
		
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{pearson.png}
\caption{Pearson Correlation Formula for a sample}
\label{overflow}
\end{figure}

In english, it is defined as the covariance ( how random variables change together ) of two sets of data, divided by their standard deviations. 
	  
		  \todo{ graph on accuracy}
		  \subsubsection{Cosine Similarity}
		  \todo{ graph on accuracy}	  
		  Cosine sim slower to evaluate than Pearson and set intersection	
		  \subsubsection{Haversine Distance}
		  \subsubsection{Geofence}
	\subsection{Data structures}
	sparse matrix, heap, dictionary, SQL database, nparray
	\subsection{Language and packages}
	Python, NumPy, SciPy, sci-kit learn
	\subsection{Problems Encountered}
		\subsubsection{Distance metric with location values}
			chose euclidean over haversine first. Worked with small values, but not a real metric for location data
		\subsubsection{Data not complete}
			location not in some entries, artist or songname not in others.	
\pagebreak
\section{Testing and Evaluation}
	\subsection{Testing Collaborative Filtering Recommenders}
		\subsubsection{Hold out}
	\subsection{Testing Location based Filtering Recommenders}
		\subsubsection{Silhouette Coefficient}
			Higher silhouette Coefficient means better clustering
		\subsubsection{GPS Visualiser}
			Plot gps coords for a user and check against output of clustering algorithm to make sure it is plotting the points correctly and finding clusters effectively.	
\pagebreak

\section{Conclusion}
	\subsection{Project Conclusion}
	\subsection{Personal Conclusion}
	
\pagebreak
\section{Future Work}
	
	\subsection{User testing}
	To combat push down and dead data problems
	
	\subsection{Mobile Application}
		An app that could have all this functionality would definitely be of use to alot of people these days.

\bibliographystyle{plain}
\bibliography{FinalYearReportJamesMurphy}
\end{document}

